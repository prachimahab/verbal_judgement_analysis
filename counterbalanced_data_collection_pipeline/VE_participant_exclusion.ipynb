{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Exclusion and Replacement Pipeline\n",
    "\n",
    "1. The counterbalancing.csv contains every json that needs to be sampled for the discrimination and verbal judgement experiment (includes all durations) \n",
    "    - The row number for each sequence corresponds to the url fragment used in the variables file uploaded to Mechanical Turk \n",
    "    - This file does not change, the variables files is updated to resample sequences that get excluded\n",
    "2. First need to match up reported participant worker IDs to worker IDs reported in batch data \n",
    "    - All data files downloaded from the server need to be matched to a worker ID in batch data\n",
    "    - Data files that do not have a matched worker ID are moved to a seperate folder and are not analyzed \n",
    "3. Participant exclusion criteron are pre-registered on OSF (link) - if the participant is excluded, the counterbalanced sequence needs to be replaced in the variables file \n",
    "4. All participants who have completed need to be excluded from completing future HITs (exclude_workers.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all Worker IDs from Batch data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_path = '/Users/pmahableshwarkar/Downloads/all_batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AZNIEFUIVB2H0',\n",
       " 'A3O9NIH7BH537H',\n",
       " 'AH7Z2M3KSQ4DW',\n",
       " 'A23KAJRDVCVGOE',\n",
       " 'A1J39RAV7TKEMF',\n",
       " 'A3G5IPGLH1IIZN',\n",
       " 'AX8NXTT8QMGHC',\n",
       " 'A3EIV1GTJ3Z2OG',\n",
       " 'A3J8UC84NM958L']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_batch_worker_ids = []\n",
    "for path in os.listdir(batch_path):   \n",
    "    if 'csv' in path:\n",
    "        batch_data = pd.read_csv(batch_path + '/' + path)\n",
    "        batch_worker_ids = list(batch_data['WorkerId'])\n",
    "        all_batch_worker_ids += batch_worker_ids\n",
    "len(all_batch_worker_ids)\n",
    "all_batch_worker_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all Worker IDs from Data Files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A3J8UC84NM958L',\n",
       " 'AX8NXTT8QMGHC',\n",
       " 'A3EIV1GTJ3Z2OG',\n",
       " 'A3O9NIH7BH537H',\n",
       " 'AZNIEFUIVB2H0',\n",
       " 'A23KAJRDVCVGOE',\n",
       " 'A3G5IPGLH1IIZN',\n",
       " 'A1J39RAV7TKEMF',\n",
       " 'AH7Z2M3KSQ4DW']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = '/Users/pmahableshwarkar/Downloads/125ms'\n",
    "\n",
    "worker_ids_from_data = []\n",
    "\n",
    "for file in os.listdir(datapath):\n",
    "    if 'csv' in file:\n",
    "        path = datapath + \"/\" + file\n",
    "        df = pd.read_csv(path, index_col=None, header=0)\n",
    "        worker_ids_from_data.append(df.workerId.unique()[0])\n",
    "\n",
    "worker_ids_from_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(worker_ids_from_data) == set(all_batch_worker_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the worker IDs that are in the data but NOT in the batch data\n",
    "# these data files should be moved to an archive and NOT analyzed\n",
    "batchdata_workerIDs = set(all_batch_worker_ids)\n",
    "missing_wid = [wid for wid in worker_ids_from_data if wid not in batchdata_workerIDs]\n",
    "missing_wid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCSVs(datafolder):\n",
    "    \"\"\"\n",
    "    Combine all participant data into one pandas df\n",
    "    OR \n",
    "    Create df for single participant file \n",
    "    \"\"\"\n",
    "        \n",
    "    #checks if path is a file\n",
    "    isFile = os.path.isfile(datafolder)\n",
    "\n",
    "    #checks if path is a directory\n",
    "    isDirectory = os.path.isdir(datafolder)\n",
    "    \n",
    "    if isDirectory == True:\n",
    "        data = []\n",
    "        for filename in os.listdir(datafolder):\n",
    "            if 'csv' in filename:\n",
    "                path = datafolder + \"/\" + filename\n",
    "                df = pd.read_csv(path, index_col=None, header=0)\n",
    "                data.append(df)\n",
    "\n",
    "        input_frame = pd.concat(data, axis=0, ignore_index=True)\n",
    "        \n",
    "    if isFile == True:\n",
    "        if 'csv' in datafolder:\n",
    "            input_frame = pd.read_csv(datafolder, index_col=None, header=0)\n",
    "    \n",
    "    print('Number of participants before cleaning: ', len(input_frame.subjID.unique()))\n",
    " \n",
    "    return input_frame\n",
    "\n",
    "def feet_to_meters(ft):\n",
    "    \"\"\"\n",
    "    Args: ft = float value in feet \n",
    "        \n",
    "    Returns: m = float value converted to meters \n",
    "    \"\"\"\n",
    "    m = ft * 0.3048\n",
    "    return m\n",
    "\n",
    "def getUnitConveredData(datafolder):\n",
    "    '''\n",
    "    Convert all responses given in feet to meters \n",
    "    '''\n",
    "    input_data = combineCSVs(datafolder) # combine CSVs from all participants \n",
    "    \n",
    "    for idx, row in input_data.iterrows():\n",
    "        unit = row['unitSelection']\n",
    "        # if estimate was made in feet, convert to meters \n",
    "        if unit == 'feet':\n",
    "            estim_ft = row['depth_estimate']\n",
    "            estim_m = feet_to_meters(estim_ft)\n",
    "            # update depth estimates in existing dataframe\n",
    "            input_data.at[idx, 'depth_estimate'] = estim_m\n",
    "            # update units in existing dataframe\n",
    "#             input_data.at[idx, 'unitSelection'] = 'meters'\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "def cleanAgeResponses(datafolder):\n",
    "    '''\n",
    "    Participants on MTurk must be over 18\n",
    "    - If participants report they are < 18, exclude from analysis \n",
    "    \n",
    "    NEED TO CHANGE HOW AGE IS COLLECTED\n",
    "    \n",
    "    '''\n",
    "    input_data = getUnitConveredData(datafolder)\n",
    "    exclude = []\n",
    "    \n",
    "    for idx, row in input_data.iterrows():\n",
    "        age = row['age']\n",
    "        # if year of birth was given, convert to age\n",
    "#         if age > 1920:\n",
    "#             actual_age = 2022-age\n",
    "#             # update age in existing dataframe\n",
    "#             input_data.at[idx, 'age'] = actual_age\n",
    "        # participants must be over 18 so age reports below 18 are junk \n",
    "        if age < 18:\n",
    "            # CHANGE THIS TO WORKER ID LATER \n",
    "            exclude.append(row['subjID'])\n",
    "    print('Number of participants excluded due to age: ', len(exclude))\n",
    "    return input_data, exclude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants before cleaning:  9\n",
      "Number of participants excluded due to age:  0\n"
     ]
    }
   ],
   "source": [
    "# data_path = '/Users/prachi/Documents/depth_duration/target_at_center/january2022_data/VE_data'\n",
    "data_path =  '/Users/pmahableshwarkar/Downloads/125ms'\n",
    "\n",
    "age_cleaned_data, exclude = cleanAgeResponses(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catchTrial_cleaning(df, correct_requirement, catch_stimuli, sequence_count, exclude):\n",
    "    '''\n",
    "    Participants complete 8 catch trials total to ensure that they are doing the task.\n",
    "    If less than 7/8 catch trials are correct, the participant is excluded.  \n",
    "    '''\n",
    "    \n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    remove = []\n",
    "    subj_sequence = {}\n",
    "    df2_list = []\n",
    "    \n",
    "    for subj in all_subjIDs:\n",
    "#         print(subj)\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning\n",
    "        subj_sequence[subj] = subj_df.sequenceName.unique()[0]\n",
    "        \n",
    "        count_correct = 0\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            stim = row['stimulus']\n",
    "            if type(stim) == str:\n",
    "                if stim.split('/')[1] in catch_stimuli:\n",
    "    #                 print(stim.split('/')[1])\n",
    "                    ####### VERSION WHERE CATCH TRIALS ARE ATTENTION CHECK: IMAGE HAS NO TARGET\n",
    "#                     print(row['depth_estimate'])\n",
    "#                     print(row['stimulus'])\n",
    "                    if row[\"depth_estimate\"] == 0:\n",
    "                        count_correct += 1\n",
    "\n",
    "                    # remove catch trial \n",
    "                    cleaned_subj_df.drop([idx], inplace=True)\n",
    "\n",
    "        if count_correct < correct_requirement:\n",
    "            remove.append(subj)\n",
    "        else:\n",
    "            sequence_count[subj_df.sequenceName.unique()[0]] += 1\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "    print(\"Number of participants that did not pass the catch trial check:\", len(remove))\n",
    "    print(\"Participants that were removed:\",remove)\n",
    "\n",
    "    for index, row in df2.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df2.drop(index, inplace=True)\n",
    "    \n",
    "    # add the list of participants to be removed to the existing list of excluded participants\n",
    "    exclude += remove \n",
    "    \n",
    "    return df2, exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/jsons'\n",
    "sequences_path = '/Users/pmahableshwarkar/Downloads/all_seqs'\n",
    "sequences_count_dict = {}\n",
    "for seq in os.listdir(sequences_path):\n",
    "    if 'json' in seq:\n",
    "        sequences_count_dict['jsons/'+seq] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_catch_stim = ['000375_2014-06-08_11-17-29_260595134347_rgbf000133-resize_2',\n",
    "                  '000569_2014-06-09_22-51-47_260595134347_rgbf000141-resize_3',\n",
    "                  '000787_2014-06-08_22-33-53_260595134347_rgbf000175-resize_1',\n",
    "                  '002072_2014-06-24_21-48-06_260595134347_rgbf000115-resize_0',\n",
    "                  '001170_2014-06-17_15-43-44_260595134347_rgbf000096-resize_6',\n",
    "                  '001222_2014-06-17_16-24-06_260595134347_rgbf000073-resize_0',\n",
    "                  '001498_2014-06-19_17-45-14_260595134347_rgbf000129-resize_4',\n",
    "                  '001540_2014-06-20_17-01-05_260595134347_rgbf000086-resize_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants that did not pass the catch trial check: 3\n",
      "Participants that were removed: [959701, 713277, 602202]\n"
     ]
    }
   ],
   "source": [
    "catch_trial_cleaned_data, exclude = catchTrial_cleaning(age_cleaned_data, 6, all_catch_stim, sequences_count_dict, exclude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeMissedTrials(df, exclude, num_trials):\n",
    "    \"\"\"\n",
    "    Participants were told that if they missed a trial, to respond '0'.\n",
    "    This function removes those trials, and keeps track of:\n",
    "    (1) How many missed trials per participant\n",
    "    (2) Number of missed trials per duration \n",
    "    (3) Number of missed trials per sequence \n",
    "    \"\"\"\n",
    "    \n",
    "    missedTrials_participants = {}\n",
    "    missedTrials_durations = {}\n",
    "    missedTrials_sequences = {}\n",
    "    \n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        estimate = row['depth_estimate']\n",
    "        if estimate == 0.0:\n",
    "            subjID = row['subjID']\n",
    "            duration = row['duration']\n",
    "            sequenceName = row['sequenceName']\n",
    "            \n",
    "            if subjID not in missedTrials_participants:\n",
    "                missedTrials_participants[subjID] = 1\n",
    "            else:\n",
    "                missedTrials_participants[subjID] += 1\n",
    "\n",
    "            if duration not in missedTrials_durations:\n",
    "                missedTrials_durations[duration] = 1\n",
    "            else:\n",
    "                missedTrials_durations[duration] += 1\n",
    "            \n",
    "            if sequenceName not in missedTrials_sequences:\n",
    "                missedTrials_sequences[sequenceName] = 1\n",
    "            else:\n",
    "                missedTrials_sequences[sequenceName] += 1\n",
    "                        \n",
    "            # remove trials with depth estimate = 0 \n",
    "            df.drop(idx, inplace=True)\n",
    "    \n",
    "    # remove participants data if the participant's missed trial count is 10% or more of num_trials\n",
    "    threshold = math.floor(num_trials * 0.1)\n",
    "    remove_ids = []\n",
    "    for key in missedTrials_participants:\n",
    "        if missedTrials_participants[key] >= threshold:\n",
    "            remove_ids.append(key)\n",
    "    print(\"Number of participants with 10% or more missed trials: \", len(remove_ids))\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['subjID'] in remove_ids:\n",
    "            df.drop(index, inplace=True)\n",
    "            \n",
    "    exclude += remove_ids\n",
    "    \n",
    "    return df, exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of meaningful trials (excludes catch-trials)\n",
    "num_trials = 156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants with 10% or more missed trials:  0\n"
     ]
    }
   ],
   "source": [
    "missed_trial_cleaned_data, exclude = removeMissedTrials(catch_trial_cleaned_data,exclude, num_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RT_Cleaning(df, exclude, outlier_range, num_trials):\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    df2_list = []\n",
    "    for subj in all_subjIDs:\n",
    "        count = 0\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning \n",
    "        # calculate subject's average trial RT \n",
    "        average_trial_RT = subj_df[\"trial_RT\"].mean()\n",
    "        std_trial_RT = subj_df[\"trial_RT\"].std()\n",
    "\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            RT = row[\"trial_RT\"]\n",
    "            if RT < outlier_range[0]: # outlier\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "            if RT > outlier_range[1]:\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "                \n",
    "        threshold = math.floor(num_trials * 0.1)\n",
    "        if count >= threshold:\n",
    "            remove.append(subj)\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "            \n",
    "    print(\"Number of Participants with 10% or more trials outside their RT range: \", len(remove))\n",
    "    \n",
    "    for index, row in df2.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df2.drop(index, inplace=True)\n",
    "            \n",
    "    exclude += remove\n",
    "    \n",
    "    return df2, exclude\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Participants with 10% or more trials outside their RT range:  1\n"
     ]
    }
   ],
   "source": [
    "RT_cleaned_data, exclude = RT_Cleaning(missed_trial_cleaned_data, exclude,[250, 10000], num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeatResponses_Cleaning(df, exclude):\n",
    "    \"\"\"\n",
    "    Some participants gave'junk data' - same number repeated for many trials \n",
    "    Count the frequency of unique responses entered by the participant. \n",
    "    If you look at the maximum number of repeats and/or the number of unique responses / 48 per participant you will find our vandals.\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    max_repeats_distribution = []\n",
    "    num_unique_responses_distribution = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        # ideally, the max repeats and num_unique_responses should be ~ 48 since there are 48 imgs at each depth bin \n",
    "        count_depth_estimates = subj_df['depth_estimate'].value_counts()\n",
    "        num_unique_responses = len(count_depth_estimates)\n",
    "        num_unique_responses_distribution.append(num_unique_responses)\n",
    "        max_repeats = count_depth_estimates.max()\n",
    "        max_repeats_distribution.append(max_repeats)\n",
    "        if num_unique_responses < 6:\n",
    "            remove.append(subj)\n",
    "    \n",
    "    avg_max_repeats = np.array(max_repeats_distribution).mean()\n",
    "    std_max_repeats = np.array(max_repeats_distribution).std()\n",
    "    \n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        count_depth_estimates = subj_df['depth_estimate'].value_counts()\n",
    "        max_repeats = count_depth_estimates.max()\n",
    "\n",
    "        outlierrange = [avg_max_repeats - (3*std_max_repeats), avg_max_repeats + (3*std_max_repeats)]\n",
    "        if max_repeats < outlierrange[0]:\n",
    "            if subj not in remove:\n",
    "                remove.append(subj)\n",
    "        if max_repeats > outlierrange[1]:\n",
    "            if subj not in remove:\n",
    "                remove.append(subj)\n",
    "                \n",
    "    print(\"Number of participants removed: repeat responses: \", len(remove))\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df.drop(index, inplace=True)\n",
    "\n",
    "    exclude += remove\n",
    "    \n",
    "    return df, max_repeats_distribution, num_unique_responses_distribution, exclude\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants removed: repeat responses:  0\n"
     ]
    }
   ],
   "source": [
    "repeat_resp_cleaned_data, max_repeats_distrib, num_unique_distrib, exclude = repeatResponses_Cleaning(RT_cleaned_data, exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalTrialCountCheck(df, exclude, num_trials):\n",
    "    \"\"\"\n",
    "    If more then 10% of a participants data is missing, remove the participant\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        count_trials = len(subj_df.index)\n",
    "        threshold_trials_remaining = num_trials - math.floor(num_trials * 0.1)\n",
    "\n",
    "        if count_trials <= threshold_trials_remaining:\n",
    "            remove.append(subj)\n",
    "            \n",
    "    print(\"Number of Participants with >= 10% trials removed: \", len(remove))\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df.drop(index, inplace=True)\n",
    "            \n",
    "    exclude += remove\n",
    "        \n",
    "    print(\"Number of participants left: \",len(df.subjID.unique()))\n",
    "    print(\"Number of participants excluded: \",len(exclude))\n",
    "    return df, exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Participants with >= 10% trials removed:  0\n",
      "Number of participants left:  3\n",
      "Number of participants excluded:  33\n"
     ]
    }
   ],
   "source": [
    "cleaned_data, exclude = finalTrialCountCheck(repeat_resp_cleaned_data, exclude, num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences_for_replacement(exclude, datafolder):\n",
    "    \n",
    "    og_data = combineCSVs(datafolder)\n",
    "    \n",
    "    seqs_to_be_replaced = []\n",
    "    for subjID in exclude:\n",
    "        subjdf = og_data.loc[og_data['subjID'] == subjID]\n",
    "        seqs_to_be_replaced.append(subjdf.sequenceName.unique()[0])\n",
    "    \n",
    "    return seqs_to_be_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants before cleaning:  9\n"
     ]
    }
   ],
   "source": [
    "sequences_to_replace = get_sequences_for_replacement(exclude, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/counterbalancing_complete.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-c473fbef71a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcounterbalancing_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/counterbalancing_complete.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcounterbalancing_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounterbalancing_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcounterbalancing_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/counterbalancing_complete.csv'"
     ]
    }
   ],
   "source": [
    "counterbalancing_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/counterbalancing_complete.csv'\n",
    "counterbalancing_df = pd.read_csv(counterbalancing_path)\n",
    "counterbalancing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jsons/VE1000_randls_31.json'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_to_replace[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_fragments = []\n",
    "for sequence in sequences_to_replace:\n",
    "    url_fragments.append(counterbalancing_df.index[counterbalancing_df['Path']==sequence][0] + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number for the NEXT batch \n",
    "batch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_variables_csv = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/counterbalanced_data_collection_pipeline/'\n",
    "\n",
    "base_url = 'http://54.235.29.9/FacialAge/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/v2_DepthDuration_HTML.html#'\n",
    "\n",
    "variables = {'experiment_url': []}\n",
    "\n",
    "for fragment in url_fragments:\n",
    "     variables['experiment_url'].append(base_url + str(fragment))\n",
    "\n",
    "variables_df = pd.DataFrame(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_df.to_csv(dest_variables_csv + 'depth_duration_variables' + '_' + str(batch) + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
