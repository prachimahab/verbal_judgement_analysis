{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Exclusion and Replacement Pipeline\n",
    "\n",
    "1. The counterbalancing.csv contains every json that needs to be sampled for the discrimination and verbal judgement experiment (includes all durations) \n",
    "    - The row number for each sequence corresponds to the url fragment used in the variables file uploaded to Mechanical Turk \n",
    "    - This file does not change, the variables files is updated to resample sequences that get excluded\n",
    "2. First need to match up reported participant worker IDs to worker IDs reported in batch data \n",
    "    - All data files downloaded from the server need to be matched to a worker ID in batch data\n",
    "    - Data files that do not have a matched worker ID are moved to a seperate folder and are not analyzed \n",
    "3. Participant exclusion criteron are pre-registered on OSF (link) - if the participant is excluded, the counterbalanced sequence needs to be replaced in the variables file \n",
    "4. All participants who have completed need to be excluded from completing future HITs (exclude_workers.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all Worker IDs from Batch data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/all_batch'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816\n"
     ]
    }
   ],
   "source": [
    "all_batch_worker_ids = []\n",
    "for path in os.listdir(batch_path):   \n",
    "    if 'csv' in path:\n",
    "        batch_data = pd.read_csv(batch_path + '/' + path)\n",
    "        batch_worker_ids = list(batch_data['WorkerId'])\n",
    "        all_batch_worker_ids += batch_worker_ids\n",
    "print(len(all_batch_worker_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all Worker IDs from Data Files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/data'\n",
    "\n",
    "worker_ids_from_data = []\n",
    "workerid_filename_dict = {}\n",
    "\n",
    "for file in os.listdir(datapath):\n",
    "    if 'csv' in file:\n",
    "        path = datapath + \"/\" + file\n",
    "        df = pd.read_csv(path, index_col=None, header=0)\n",
    "        worker_ids_from_data.append(df.workerId.unique()[0])\n",
    "        workerid_filename_dict[df.workerId.unique()[0]] = file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(worker_ids_from_data) == set(all_batch_worker_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# get the worker IDs that are in the data but NOT in the batch data\n",
    "# these data files should be moved to an archive and NOT analyzed\n",
    "batchdata_workerIDs = set(all_batch_worker_ids)\n",
    "move_files = []\n",
    "missing_wid = [wid for wid in worker_ids_from_data if wid not in batchdata_workerIDs]\n",
    "print(len(missing_wid))\n",
    "for wid in missing_wid:\n",
    "    move_files.append(workerid_filename_dict[wid])\n",
    "print(len(move_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8-29-2022_13-7-14_137123.csv', '8-29-2022_11-34-58_451259.csv']"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/data'\n",
    "dest_dir = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/data_archive'\n",
    "\n",
    "for file in move_files:\n",
    "    shutil.move(current_dir + '/' + file, dest_dir + '/' + file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCSVs(datafolder):\n",
    "    \"\"\"\n",
    "    Combine all participant data into one pandas df\n",
    "    OR \n",
    "    Create df for single participant file \n",
    "    \"\"\"\n",
    "        \n",
    "    #checks if path is a file\n",
    "    isFile = os.path.isfile(datafolder)\n",
    "\n",
    "    #checks if path is a directory\n",
    "    isDirectory = os.path.isdir(datafolder)\n",
    "    \n",
    "    if isDirectory == True:\n",
    "        data = []\n",
    "        for filename in os.listdir(datafolder):\n",
    "            if 'csv' in filename:\n",
    "                path = datafolder + \"/\" + filename\n",
    "                df = pd.read_csv(path, index_col=None, header=0)\n",
    "                data.append(df)\n",
    "\n",
    "        input_frame = pd.concat(data, axis=0, ignore_index=True)\n",
    "        \n",
    "    if isFile == True:\n",
    "        if 'csv' in datafolder:\n",
    "            input_frame = pd.read_csv(datafolder, index_col=None, header=0)\n",
    "    \n",
    "    print('Number of participants before cleaning: ', len(input_frame.subjID.unique()))\n",
    " \n",
    "    return input_frame\n",
    "\n",
    "def feet_to_meters(ft):\n",
    "    \"\"\"\n",
    "    Args: ft = float value in feet \n",
    "        \n",
    "    Returns: m = float value converted to meters \n",
    "    \"\"\"\n",
    "    m = ft * 0.3048\n",
    "    return m\n",
    "\n",
    "def getUnitConveredData(datafolder):\n",
    "    '''\n",
    "    Convert all responses given in feet to meters \n",
    "    '''\n",
    "    input_data = combineCSVs(datafolder) # combine CSVs from all participants \n",
    "    \n",
    "    for idx, row in input_data.iterrows():\n",
    "        unit = row['unitSelection']\n",
    "        # if estimate was made in feet, convert to meters \n",
    "        if unit == 'feet':\n",
    "            estim_ft = row['depth_estimate']\n",
    "            estim_m = feet_to_meters(estim_ft)\n",
    "            # update depth estimates in existing dataframe\n",
    "            input_data.at[idx, 'depth_estimate'] = estim_m\n",
    "            # update units in existing dataframe\n",
    "#             input_data.at[idx, 'unitSelection'] = 'meters'\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "def cleanAgeResponses(datafolder):\n",
    "    '''\n",
    "    Participants on MTurk must be over 18\n",
    "    - If participants report they are < 18, exclude from analysis \n",
    "    \n",
    "    NEED TO CHANGE HOW AGE IS COLLECTED\n",
    "    \n",
    "    '''\n",
    "    input_data = getUnitConveredData(datafolder)\n",
    "    exclude = []\n",
    "    \n",
    "    for idx, row in input_data.iterrows():\n",
    "        age = row['age']\n",
    "    print('Number of participants excluded due to age: ', len(exclude))\n",
    "    return input_data, exclude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants before cleaning:  252\n",
      "Number of participants excluded due to age:  0\n"
     ]
    }
   ],
   "source": [
    "# data_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/data'\n",
    "\n",
    "data_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/data_b6'\n",
    "\n",
    "age_cleaned_data, exclude = cleanAgeResponses(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catchTrial_cleaning(df, correct_requirement, catch_stimuli, sequence_count, exclude):\n",
    "    '''\n",
    "    Participants complete 8 catch trials total to ensure that they are doing the task.\n",
    "    If less than 7/8 catch trials are correct, the participant is excluded.  \n",
    "    '''\n",
    "    \n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    remove = []\n",
    "    subj_sequence = {}\n",
    "    df2_list = []\n",
    "    \n",
    "    for subj in all_subjIDs:\n",
    "#         print(subj)\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning\n",
    "        subj_sequence[subj] = subj_df.sequenceName.unique()[0]\n",
    "        \n",
    "        count_correct = 0\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            stim = row['stimulus']\n",
    "            if type(stim) == str:\n",
    "                if stim.split('/')[1] in catch_stimuli:\n",
    "                    ####### VERSION WHERE CATCH TRIALS ARE ATTENTION CHECK: IMAGE HAS NO TARGET\n",
    "#                     print(row['depth_estimate'])\n",
    "#                     print(row['stimulus'])\n",
    "                    if row[\"depth_estimate\"] == 0:\n",
    "                        count_correct += 1\n",
    "\n",
    "                    # remove catch trial \n",
    "                    cleaned_subj_df.drop([idx], inplace=True)\n",
    "\n",
    "        if count_correct < correct_requirement:\n",
    "            remove.append(subj)\n",
    "        else:\n",
    "            sequence_count[subj_df.sequenceName.unique()[0]] += 1\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "    print(\"Number of participants that did not pass the catch trial check:\", len(remove))\n",
    "    print(\"Participants that were removed:\",remove)\n",
    "\n",
    "    for index, row in df2.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df2.drop(index, inplace=True)\n",
    "    \n",
    "    # add the list of participants to be removed to the existing list of excluded participants\n",
    "    exclude += remove \n",
    "    \n",
    "    return df2, exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/jsons'\n",
    "sequences_count_dict = {}\n",
    "for seq in os.listdir(sequences_path):\n",
    "    if 'json' in seq:\n",
    "        sequences_count_dict['jsons/'+seq] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_catch_stim = ['000375_2014-06-08_11-17-29_260595134347_rgbf000133-resize_2',\n",
    "                  '000569_2014-06-09_22-51-47_260595134347_rgbf000141-resize_3',\n",
    "                  '000787_2014-06-08_22-33-53_260595134347_rgbf000175-resize_1',\n",
    "                  '002072_2014-06-24_21-48-06_260595134347_rgbf000115-resize_0',\n",
    "                  '001170_2014-06-17_15-43-44_260595134347_rgbf000096-resize_6',\n",
    "                  '001222_2014-06-17_16-24-06_260595134347_rgbf000073-resize_0',\n",
    "                  '001498_2014-06-19_17-45-14_260595134347_rgbf000129-resize_4',\n",
    "                  '001540_2014-06-20_17-01-05_260595134347_rgbf000086-resize_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants that did not pass the catch trial check: 72\n",
      "Participants that were removed: [199719, 552070, 626250, 121796, 980305, 252260, 634447, 465507, 653026, 686124, 158205, 729944, 837088, 739896, 644462, 603293, 506066, 492147, 757584, 336867, 970981, 910393, 666910, 866086, 512637, 354578, 449004, 638287, 793228, 453024, 812610, 636347, 658248, 789736, 743554, 126667, 740754, 232259, 679854, 123268, 240085, 166671, 701497, 481815, 975469, 645545, 560512, 556968, 777408, 987560, 725003, 471013, 456982, 175604, 234372, 527750, 521117, 974329, 625078, 945231, 441122, 236276, 746280, 680594, 189617, 279337, 298143, 708772, 870813, 728200, 693109, 835910]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catch_trial_cleaned_data, exclude = catchTrial_cleaning(age_cleaned_data, 6, all_catch_stim, sequences_count_dict, exclude)\n",
    "len(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeMissedTrials(df, exclude, num_trials):\n",
    "    \"\"\"\n",
    "    Participants were told that if they missed a trial, to respond '0'.\n",
    "    This function removes those trials, and keeps track of:\n",
    "    (1) How many missed trials per participant\n",
    "    (2) Number of missed trials per duration \n",
    "    (3) Number of missed trials per sequence \n",
    "    \"\"\"\n",
    "    \n",
    "    missedTrials_participants = {}\n",
    "    missedTrials_durations = {}\n",
    "    missedTrials_sequences = {}\n",
    "    \n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        estimate = row['depth_estimate']\n",
    "        if estimate == 0.0:\n",
    "            subjID = row['subjID']\n",
    "            duration = row['duration']\n",
    "            sequenceName = row['sequenceName']\n",
    "            \n",
    "            if subjID not in missedTrials_participants:\n",
    "                missedTrials_participants[subjID] = 1\n",
    "            else:\n",
    "                missedTrials_participants[subjID] += 1\n",
    "\n",
    "            if duration not in missedTrials_durations:\n",
    "                missedTrials_durations[duration] = 1\n",
    "            else:\n",
    "                missedTrials_durations[duration] += 1\n",
    "            \n",
    "            if sequenceName not in missedTrials_sequences:\n",
    "                missedTrials_sequences[sequenceName] = 1\n",
    "            else:\n",
    "                missedTrials_sequences[sequenceName] += 1\n",
    "                        \n",
    "            # remove trials with depth estimate = 0 \n",
    "            df.drop(idx, inplace=True)\n",
    "    \n",
    "    # remove participants data if the participant's missed trial count is 10% or more of num_trials\n",
    "    threshold = math.floor(num_trials * 0.1)\n",
    "    remove_ids = []\n",
    "    for key in missedTrials_participants:\n",
    "        if missedTrials_participants[key] >= threshold:\n",
    "            remove_ids.append(key)\n",
    "    print(\"Number of participants with 10% or more missed trials: \", len(remove_ids))\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['subjID'] in remove_ids:\n",
    "            df.drop(index, inplace=True)\n",
    "            \n",
    "    exclude += remove_ids\n",
    "    \n",
    "    return df, exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of meaningful trials (excludes catch-trials)\n",
    "num_trials = 156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants with 10% or more missed trials:  24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_trial_cleaned_data, exclude = removeMissedTrials(catch_trial_cleaned_data,exclude, num_trials)\n",
    "len(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RT_Cleaning(df, exclude, outlier_range, num_trials):\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    df2_list = []\n",
    "    short = 0\n",
    "    long = 0\n",
    "    for subj in all_subjIDs:\n",
    "        count = 0\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning \n",
    "        # calculate subject's average trial RT \n",
    "        average_trial_RT = subj_df[\"trial_RT\"].mean()\n",
    "        std_trial_RT = subj_df[\"trial_RT\"].std()\n",
    "        \n",
    "        \n",
    "        for idx, row in subj_df.iterrows():\n",
    "            RT = row[\"trial_RT\"]\n",
    "            if RT < outlier_range[0]: # outlier\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "                short += 1\n",
    "            if RT > outlier_range[1]:\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "                long += 1\n",
    "                \n",
    "        threshold = math.floor(num_trials * 0.1)\n",
    "        if count >= threshold:\n",
    "            remove.append(subj)\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "            \n",
    "    print(\"Number of Participants with 10% or more trials outside their RT range: \", len(remove))\n",
    "    \n",
    "    for index, row in df2.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df2.drop(index, inplace=True)\n",
    "            \n",
    "    exclude += remove\n",
    "    \n",
    "    print(short, long)\n",
    "    return df2, exclude\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Participants with 10% or more trials outside their RT range:  18\n",
      "0 907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RT_cleaned_data, exclude = RT_Cleaning(missed_trial_cleaned_data, exclude,[250, 10000], num_trials)\n",
    "len(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeatResponses_Cleaning(df, exclude):\n",
    "    \"\"\"\n",
    "    Some participants gave'junk data' - same number repeated for many trials \n",
    "    Count the frequency of unique responses entered by the participant. \n",
    "    If you look at the maximum number of repeats and/or the number of unique responses / 48 per participant you will find our vandals.\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    max_repeats_distribution = []\n",
    "    num_unique_responses_distribution = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        # ideally, the max repeats and num_unique_responses should be ~ 48 since there are 48 imgs at each depth bin \n",
    "        count_depth_estimates = subj_df['depth_estimate'].value_counts()\n",
    "        num_unique_responses = len(count_depth_estimates)\n",
    "        num_unique_responses_distribution.append(num_unique_responses)\n",
    "        max_repeats = count_depth_estimates.max()\n",
    "        max_repeats_distribution.append(max_repeats)\n",
    "        if num_unique_responses < 6:\n",
    "            remove.append(subj)\n",
    "    \n",
    "    avg_max_repeats = np.array(max_repeats_distribution).mean()\n",
    "    std_max_repeats = np.array(max_repeats_distribution).std()\n",
    "    \n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        count_depth_estimates = subj_df['depth_estimate'].value_counts()\n",
    "        max_repeats = count_depth_estimates.max()\n",
    "\n",
    "        outlierrange = [avg_max_repeats - (3*std_max_repeats), avg_max_repeats + (3*std_max_repeats)]\n",
    "        if max_repeats < outlierrange[0]:\n",
    "            if subj not in remove:\n",
    "                remove.append(subj)\n",
    "        if max_repeats > outlierrange[1]:\n",
    "            if subj not in remove:\n",
    "                remove.append(subj)\n",
    "                \n",
    "    print(\"Number of participants removed: repeat responses: \", len(remove))\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df.drop(index, inplace=True)\n",
    "\n",
    "    exclude += remove\n",
    "    \n",
    "    return df, max_repeats_distribution, num_unique_responses_distribution, exclude\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants removed: repeat responses:  20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_resp_cleaned_data, max_repeats_distrib, num_unique_distrib, exclude = repeatResponses_Cleaning(RT_cleaned_data, exclude)\n",
    "len(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalTrialCountCheck(df, exclude, num_trials):\n",
    "    \"\"\"\n",
    "    If more then 10% of a participants data is missing, remove the participant\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        count_trials = len(subj_df.index)\n",
    "        threshold_trials_remaining = num_trials - math.floor(num_trials * 0.1)\n",
    "\n",
    "        if count_trials <= threshold_trials_remaining:\n",
    "            remove.append(subj)\n",
    "            \n",
    "    print(\"Number of Participants with >= 10% trials removed: \", len(remove))\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df.drop(index, inplace=True)\n",
    "            \n",
    "    exclude += remove\n",
    "        \n",
    "    print(\"Number of participants left: \",len(df.subjID.unique()))\n",
    "    return df, exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Participants with >= 10% trials removed:  2\n",
      "Number of participants left:  116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data, exclude = finalTrialCountCheck(repeat_resp_cleaned_data, exclude, num_trials)\n",
    "len(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences_for_replacement(exclude, datafolder):\n",
    "    \n",
    "    og_data = combineCSVs(datafolder)\n",
    "    \n",
    "    seqs_to_be_replaced = []\n",
    "    for subjID in exclude:\n",
    "        subjdf = og_data.loc[og_data['subjID'] == subjID]\n",
    "        seqs_to_be_replaced.append(subjdf.sequenceName.unique()[0])\n",
    "    print('Number of Sequences to be replaced:', len(set(seqs_to_be_replaced)))\n",
    "        \n",
    "    return set(seqs_to_be_replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants before cleaning:  252\n",
      "Number of Sequences to be replaced: 135\n"
     ]
    }
   ],
   "source": [
    "sequences_to_replace = get_sequences_for_replacement(exclude, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sampled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jsons/v1_VE250_randls_59.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jsons/VE125_randls_39.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jsons/VE125_randls_58_rotated.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jsons/v1_VE250_randls_18.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jsons/v1_VE250_randls_36_rotated.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>jsons/VE125_randls_4.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>jsons/v1_VE250_randls_21.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>jsons/VE125_randls_39_rotated.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>jsons/VE125_randls_41.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>jsons/v1_VE1000_randls_55.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Path  Sampled\n",
       "0            jsons/v1_VE250_randls_59.json        0\n",
       "1               jsons/VE125_randls_39.json        0\n",
       "2       jsons/VE125_randls_58_rotated.json        0\n",
       "3            jsons/v1_VE250_randls_18.json        0\n",
       "4    jsons/v1_VE250_randls_36_rotated.json        0\n",
       "..                                     ...      ...\n",
       "463              jsons/VE125_randls_4.json        0\n",
       "464          jsons/v1_VE250_randls_21.json        0\n",
       "465     jsons/VE125_randls_39_rotated.json        0\n",
       "466             jsons/VE125_randls_41.json        0\n",
       "467         jsons/v1_VE1000_randls_55.json        0\n",
       "\n",
       "[468 rows x 2 columns]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterbalancing_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/counterbalancing.csv'\n",
    "counterbalancing_df = pd.read_csv(counterbalancing_path)\n",
    "counterbalancing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jsons/VE125_randls_69_rotated.json'"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_to_replace = list(sequences_to_replace)\n",
    "sequences_to_replace[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 37, 55)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1000 = 0\n",
    "count_250 = 0\n",
    "count_125 = 0\n",
    "for seq in sequences_to_replace:\n",
    "    if 'VE1000' in seq:\n",
    "        count_1000 += 1\n",
    "    if 'VE250' in seq:\n",
    "        count_250 += 1\n",
    "    if 'VE125' in seq:\n",
    "        count_125 += 1\n",
    "\n",
    "count_1000, count_250, count_125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Notes \n",
    "\n",
    "The row in the counterbalancing csv does NOT match the url fragment since the indexing includes the path row. \n",
    "\n",
    "The url fragment is the counterbalancing df index + 1 --> this has been validated in the console log of the experiment \n",
    "\n",
    "To backtrack from the url fragments to the corresponding row of the counterbalancing csv:\n",
    "row = url_fragment + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_fragments = []\n",
    "for sequence in sequences_to_replace:\n",
    "    url_fragments.append(counterbalancing_df.index[counterbalancing_df['Path']==sequence][0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number for the NEXT batch \n",
    "batch = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_variables_csv = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/mturk_batch_variables/'\n",
    "\n",
    "base_url = 'http://54.235.29.9/FacialAge/BNav_EC2/DepthDuration/v2_depth_duration_MTurk/v2_DepthDuration_HTML.html#'\n",
    "\n",
    "variables = {'experiment_url': []}\n",
    "\n",
    "for fragment in url_fragments:\n",
    "     variables['experiment_url'].append(base_url + str(fragment))\n",
    "\n",
    "variables_df = pd.DataFrame(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_df.to_csv(dest_variables_csv + 'depth_duration_variables' + '_b' + str(batch) + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
